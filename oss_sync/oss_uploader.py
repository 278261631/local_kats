"""
é˜¿é‡Œäº‘ OSS æ–‡ä»¶ä¸Šä¼ å·¥å…·
ç”¨äºå°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ åˆ°é˜¿é‡Œäº‘ OSS å­˜å‚¨ï¼ŒæŒ‰ç…§ yyyy/yyyymmdd/ çš„è·¯å¾„ç»“æ„ç»„ç»‡
"""

import os
import sys
import json
import logging
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
try:
    import oss2
except Exception as e:
    print("âœ— ç¼ºå°‘ä¾èµ–æˆ–å¯¼å…¥ oss2 å¤±è´¥ã€‚è¯·å…ˆå®‰è£…ä¾èµ–: pip install -r oss_sync/requirements.txt")
    print(f"è¯¦ç»†é”™è¯¯: {e}")
    import sys
    sys.exit(1)
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import subprocess
import shutil



class OSSUploader:
    """é˜¿é‡Œäº‘ OSS ä¸Šä¼ å™¨"""

    def __init__(self, config_file: str = "oss_config.json"):
        """
        åˆå§‹åŒ–ä¸Šä¼ å™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.config = self._load_config()
        self.logger = self._setup_logging()

        # åˆå§‹åŒ– OSS å®¢æˆ·ç«¯
        self.auth = None
        self.bucket = None
        self._init_oss_client()

    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(self.config_file):
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ¨¡æ¿åˆ›å»º
            template_file = self.config_file + ".template"
            if os.path.exists(template_file):
                print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä»æ¨¡æ¿åˆ›å»º: {self.config_file}")
                with open(template_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                with open(self.config_file, 'w', encoding='utf-8') as f:
                    json.dump(config, f, indent=2, ensure_ascii=False)
                print(f"è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶ {self.config_file} å¹¶å¡«å†™å¿…è¦çš„ä¿¡æ¯")
                sys.exit(1)
            else:
                raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_file}")

        with open(self.config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # éªŒè¯å¿…è¦çš„é…ç½®é¡¹
        required_fields = ['access_key_id', 'access_key_secret', 'bucket_name']
        for field in required_fields:
            if not config['aliyun_oss'].get(field):
                raise ValueError(f"é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦å­—æ®µ: aliyun_oss.{field}")

        if not config['upload_settings'].get('oss_root'):
            raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦å­—æ®µ: upload_settings.oss_root")

        return config

    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger('OSSUploader')
        logger.setLevel(logging.INFO)

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        # æ–‡ä»¶å¤„ç†å™¨
        log_file = Path(__file__).parent / 'oss_upload.log'
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)

        # æ ¼å¼åŒ–
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)

        logger.addHandler(console_handler)
        logger.addHandler(file_handler)

        return logger

    def _init_oss_client(self):
        """åˆå§‹åŒ– OSS å®¢æˆ·ç«¯"""
        try:
            oss_config = self.config['aliyun_oss']
            self.auth = oss2.Auth(
                oss_config['access_key_id'],
                oss_config['access_key_secret']
            )
            self.bucket = oss2.Bucket(
                self.auth,
                oss_config['endpoint'],
                oss_config['bucket_name']
            )
            self.logger.info(f"OSS å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {oss_config['bucket_name']}")
        except Exception as e:
            self.logger.error(f"OSS å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def _get_file_md5(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶ MD5"""
        md5_hash = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                md5_hash.update(chunk)
        return md5_hash.hexdigest()

    def _extract_date_from_path(self, file_path: Path) -> Optional[datetime]:
        """
        ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ—¥æœŸ
        ä¼˜å…ˆä»æ–‡ä»¶åä¸­çš„ UTC æ—¶é—´æˆ³æå–ï¼Œå¦‚æœå¤±è´¥åˆ™ä»è·¯å¾„ä¸­çš„æ—¥æœŸç›®å½•æå–

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æå–çš„æ—¥æœŸï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å› None
        """
        # æ–¹æ³•1: ä»æ–‡ä»¶åä¸­æå– UTC æ—¶é—´æˆ³
        # æ ¼å¼: GY5_K052-1_No%20Filter_60S_Bin2_UTC20251102_131726_-19.9C_
        path_str = str(file_path)

        # åŒ¹é… UTC åé¢çš„æ—¥æœŸ (YYYYMMDD)
        utc_match = re.search(r'UTC(\d{8})_', path_str)
        if utc_match:
            date_str = utc_match.group(1)
            try:
                return datetime.strptime(date_str, "%Y%m%d")
            except ValueError:
                pass

        # æ–¹æ³•2: ä»è·¯å¾„ä¸­çš„æ—¥æœŸç›®å½•æå–
        # æ ¼å¼: GY5/20251102/K052/...
        parts = file_path.parts
        for part in parts:
            # æ£€æŸ¥æ˜¯å¦æ˜¯8ä½æ•°å­—çš„æ—¥æœŸæ ¼å¼
            if re.match(r'^\d{8}$', part):
                try:
                    return datetime.strptime(part, "%Y%m%d")
                except ValueError:
                    pass

        # æ–¹æ³•3: å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å› None
        return None

    def _get_batch_date(self, oss_root: Path) -> Optional[datetime]:
        """
        ä»oss_rootç›®å½•ä¸­æå–æ‰¹æ¬¡æ—¥æœŸ
        ä¼˜å…ˆä»HTMLæ–‡ä»¶åæå–ï¼Œå…¶æ¬¡ä»ç›®å½•ç»“æ„æå–

        Args:
            oss_root: OSS æ ¹ç›®å½•

        Returns:
            æ‰¹æ¬¡æ—¥æœŸï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å› None
        """
        # æ–¹æ³•1: ä»HTMLæ–‡ä»¶åæå–æ—¥æœŸ
        # æ ¼å¼: detection_results_20251102.html
        html_files = list(oss_root.glob("detection_results_*.html"))
        if html_files:
            html_file = html_files[0]
            match = re.search(r'detection_results_(\d{8})\.html', html_file.name)
            if match:
                date_str = match.group(1)
                try:
                    return datetime.strptime(date_str, "%Y%m%d")
                except ValueError:
                    pass

        # æ–¹æ³•2: ä»ç›®å½•ç»“æ„ä¸­æå–æ—¥æœŸ
        # éå†æ‰€æœ‰æ–‡ä»¶ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ…å«æ—¥æœŸçš„è·¯å¾„
        for file_path in oss_root.rglob("*"):
            if file_path.is_file():
                file_date = self._extract_date_from_path(file_path)
                if file_date:
                    return file_date

        return None

    def _get_oss_path(self, local_file: Path, oss_root: Path, batch_date: datetime) -> str:
        """
        æ ¹æ®æ–‡ä»¶è·¯å¾„ç”Ÿæˆ OSS è·¯å¾„
        æ ¼å¼: yyyy/yyyymmdd/ç›¸å¯¹è·¯å¾„ï¼ˆåŸæ ·ä¿ç•™ï¼Œä¸å†ç§»é™¤ç›¸å¯¹è·¯å¾„ä¸­çš„æ—¥æœŸç›®å½•ï¼‰

        Args:
            local_file: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            oss_root: OSS æ ¹ç›®å½•
            batch_date: æ‰¹æ¬¡æ—¥æœŸ(æ‰€æœ‰æ–‡ä»¶ä½¿ç”¨åŒä¸€ä¸ªæ—¥æœŸ)

        Returns:
            OSS å¯¹è±¡è·¯å¾„
        """
        # è·å–ç›¸å¯¹äº oss_root çš„è·¯å¾„
        try:
            relative_path = local_file.relative_to(oss_root)
        except ValueError:
            # å¦‚æœæ–‡ä»¶ä¸åœ¨ oss_root ä¸‹ï¼Œä½¿ç”¨æ–‡ä»¶å
            relative_path = Path(local_file.name)

        # ä½¿ç”¨æ‰¹æ¬¡æ—¥æœŸ
        year = batch_date.strftime("%Y")
        date_str = batch_date.strftime("%Y%m%d")

        # è¿™é‡Œä¸å†ç§»é™¤ç›¸å¯¹è·¯å¾„ä¸­çš„æ—¥æœŸç›®å½•ï¼Œä¿æŒå¯¼å‡ºç›®å½•ç»“æ„ï¼šGY5/20251102/K021/...
        cleaned_relative_path = relative_path

        oss_path = f"{year}/{date_str}/{cleaned_relative_path.as_posix()}"

        return oss_path

    def _find_7z_executable(self) -> Optional[str]:
        """æŸ¥æ‰¾ 7z å¯æ‰§è¡Œç¨‹åº"""
        try:
            path = shutil.which("7z") or shutil.which("7z.exe")
        except Exception:
            path = None
        if path:
            return path
        common = Path("C:/Program Files/7-Zip/7z.exe")
        if common.exists():
            return str(common)
        return None

    def _create_7z_archive(self, oss_root: Path, batch_date: datetime) -> Path:
        """
        å°† oss_root ç›®å½•æ‰“åŒ…ä¸º 7z å‹ç¼©åŒ…ï¼Œæ–‡ä»¶å: oss_root_yyyymmdd.7zï¼Œå­˜æ”¾åœ¨å…¶çˆ¶ç›®å½•
        """
        date_str = batch_date.strftime("%Y%m%d")
        archive_name = f"{oss_root.name}_{date_str}.7z"
        archive_path = oss_root.parent / archive_name

        # å¦‚æœå·²å­˜åœ¨åˆ™åˆ é™¤ä»¥ä¾¿é‡å»º
        try:
            if archive_path.exists():
                self.logger.info(f"ç›®æ ‡å‹ç¼©åŒ…å·²å­˜åœ¨ï¼Œåˆ é™¤åé‡æ–°ç”Ÿæˆ: {archive_path}")
                archive_path.unlink()
        except Exception as e:
            self.logger.warning(f"æ— æ³•åˆ é™¤å·²å­˜åœ¨å‹ç¼©åŒ…ï¼Œå°†è¦†ç›–åˆ›å»º: {archive_path} - {e}")

        self.logger.info("=" * 60)
        self.logger.info(f"å¼€å§‹æ‰“åŒ…ä¸º 7z: {archive_path}")
        self.logger.info(f"æ‰“åŒ…æºç›®å½•: {oss_root}")

        # é¦–é€‰ä½¿ç”¨ py7zr
        try:
            import py7zr  # type: ignore
            with py7zr.SevenZipFile(str(archive_path), 'w') as archive:
                # ç¡®ä¿å‹ç¼©åŒ…å†…åŒ…å«é¡¶å±‚ç›®å½•å oss_root.name
                archive.writeall(str(oss_root), arcname=oss_root.name)
            self.logger.info(f"âœ“ 7z æ‰“åŒ…å®Œæˆ (py7zr): {archive_path}")
        except Exception as py7zr_err:
            self.logger.info(f"æœªèƒ½ä½¿ç”¨ py7zr æ‰“åŒ…ï¼Œå°†å°è¯•ç³»ç»Ÿ 7z å‘½ä»¤è¡Œã€‚åŸå› : {py7zr_err}")
            sevenz = self._find_7z_executable()
            if not sevenz:
                raise RuntimeError(
                    "æœªæ‰¾åˆ° py7zr æˆ– 7z å¯æ‰§è¡Œç¨‹åºã€‚è¯·å®‰è£…ä»»ä¸€å…¶ä¸€åé‡è¯•ï¼š\n"
                    "  pip install py7zr\n"
                    "æˆ–å®‰è£… 7-Zip å¹¶å°† 7z åŠ å…¥ PATH"
                ) from py7zr_err
            # åœ¨çˆ¶ç›®å½•ä¸‹æ‰§è¡Œ: 7z a -t7z archive.7z oss_root_name
            cmd = [sevenz, 'a', '-t7z', str(archive_path), oss_root.name]
            try:
                completed = subprocess.run(
                    cmd, cwd=str(oss_root.parent), capture_output=True, text=True
                )
                if completed.returncode != 0:
                    raise RuntimeError(f"7z å‘½ä»¤æ‰§è¡Œå¤±è´¥: {completed.stderr or completed.stdout}")
                self.logger.info(f" 7z æ‰“åŒ…å®Œæˆ (7z CLI): {archive_path}")
            except Exception as cli_err:
                raise RuntimeError(f"7z æ‰“åŒ…å¤±è´¥: {cli_err}") from cli_err

        # æ—¥å¿—è®°å½•æ‰“åŒ…åå¤§å°
        try:
            size_mb = archive_path.stat().st_size / (1024 * 1024)
            self.logger.info(f"å‹ç¼©åŒ…å¤§å°: {size_mb:.2f} MB")
        except Exception:
            pass

        self.logger.info("=" * 60)
        return archive_path

    def _upload_file(self, local_file: Path, oss_path: str, retry_times: int = 3) -> Dict:
        """
        ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ° OSS

        Args:
            local_file: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            oss_path: OSS å¯¹è±¡è·¯å¾„
            retry_times: é‡è¯•æ¬¡æ•°

        Returns:
            ä¸Šä¼ ç»“æœå­—å…¸: {'success': bool, 'skipped': bool, 'file': str, 'oss_path': str}
        """
        result = {
            'success': False,
            'skipped': False,
            'file': str(local_file),
            'oss_path': oss_path
        }

        for attempt in range(retry_times):
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                try:
                    remote_meta = self.bucket.get_object_meta(oss_path)
                    remote_size = int(remote_meta.headers.get('Content-Length', 0))
                    local_size = os.path.getsize(local_file)

                    if remote_size == local_size:
                        self.logger.info(f"âŠ™ è·³è¿‡(å·²å­˜åœ¨): {local_file.name}")
                        result['success'] = True
                        result['skipped'] = True
                        return result
                except oss2.exceptions.NoSuchKey:
                    # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»§ç»­ä¸Šä¼ 
                    pass

                # æ˜¾ç¤ºä¸Šä¼ å¼€å§‹ä¿¡æ¯
                file_size_mb = os.path.getsize(local_file) / (1024 * 1024)
                self.logger.info(f"â†‘ æ­£åœ¨ä¸Šä¼ : {local_file.name} ({file_size_mb:.2f} MB)")

                # ä¸Šä¼ æ–‡ä»¶
                timeout = self.config['upload_settings'].get('timeout', 300)
                self.bucket.put_object_from_file(oss_path, str(local_file),
                                                  headers={'x-oss-storage-class': 'Standard'})

                # éªŒè¯ä¸Šä¼ 
                remote_meta = self.bucket.get_object_meta(oss_path)
                remote_size = int(remote_meta.headers.get('Content-Length', 0))
                local_size = os.path.getsize(local_file)

                if remote_size == local_size:
                    self.logger.info(f"âœ“ ä¸Šä¼ æˆåŠŸ: {local_file.name} -> {oss_path}")
                    result['success'] = True
                    return result
                else:
                    self.logger.warning(f"âš  æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœ¬åœ°={local_size}, è¿œç¨‹={remote_size}")

            except Exception as e:
                self.logger.warning(f"âœ— ä¸Šä¼ å¤±è´¥ (å°è¯• {attempt + 1}/{retry_times}): {local_file.name} - {str(e)}")
                if attempt == retry_times - 1:
                    self.logger.error(f"âœ— æœ€ç»ˆå¤±è´¥: {local_file.name}")
                    return result

        return result

    def scan_files(self, root_dir: Path, extensions: List[str]) -> List[Path]:
        """
        æ‰«æç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶

        Args:
            root_dir: æ ¹ç›®å½•
            extensions: æ–‡ä»¶æ‰©å±•ååˆ—è¡¨

        Returns:
            æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        files = []
        self.logger.info("=" * 60)
        self.logger.info(f"å¼€å§‹æ‰«æç›®å½•: {root_dir}")
        self.logger.info(f"æ‰«ææ–‡ä»¶ç±»å‹: {', '.join(extensions)}")
        self.logger.info("=" * 60)

        for ext in extensions:
            pattern = f"**/*{ext}"
            self.logger.info(f"æ­£åœ¨æ‰«æ {ext} æ–‡ä»¶...")
            matched_files = list(root_dir.glob(pattern))
            files.extend(matched_files)
            if matched_files:
                # è®¡ç®—æ€»å¤§å°
                total_size = sum(f.stat().st_size for f in matched_files)
                total_size_mb = total_size / (1024 * 1024)
                self.logger.info(f"  æ‰¾åˆ° {len(matched_files)} ä¸ª {ext} æ–‡ä»¶ (æ€»å¤§å°: {total_size_mb:.2f} MB)")
            else:
                self.logger.info(f"  æœªæ‰¾åˆ° {ext} æ–‡ä»¶")

        if files:
            total_size = sum(f.stat().st_size for f in files)
            total_size_mb = total_size / (1024 * 1024)
            self.logger.info("=" * 60)
            self.logger.info(f"æ‰«æå®Œæˆ: æ€»å…±æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ (æ€»å¤§å°: {total_size_mb:.2f} MB)")
            self.logger.info("=" * 60)
        else:
            self.logger.warning("=" * 60)
            self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
            self.logger.warning("=" * 60)

        return files

    def upload_files(self, files: List[Path], oss_root: Path, max_workers: int = 4):
        """
        æ‰¹é‡ä¸Šä¼ æ–‡ä»¶

        Args:
            files: æ–‡ä»¶åˆ—è¡¨
            oss_root: OSS æ ¹ç›®å½•
            max_workers: æœ€å¤§å¹¶å‘æ•°
        """
        if not files:
            self.logger.warning("æ²¡æœ‰æ–‡ä»¶éœ€è¦ä¸Šä¼ ")
            return

        # è·å–æ‰¹æ¬¡æ—¥æœŸ
        batch_date = self._get_batch_date(oss_root)
        if batch_date is None:
            self.logger.error("âœ— æ— æ³•ä»ç›®å½•ä¸­æå–æ‰¹æ¬¡æ—¥æœŸï¼Œä¸Šä¼ ç»ˆæ­¢")
            return

        self.logger.info("=" * 60)
        self.logger.info(f"æ‰¹æ¬¡æ—¥æœŸ: {batch_date.strftime('%Y%m%d')}")
        self.logger.info(f"å¼€å§‹ä¸Šä¼  {len(files)} ä¸ªæ–‡ä»¶")
        self.logger.info(f"å¹¶å‘æ•°: {max_workers}")
        self.logger.info("=" * 60)

        success_count = 0
        failed_count = 0
        skipped_count = 0

        retry_times = self.config['upload_settings'].get('retry_times', 3)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸Šä¼ ä»»åŠ¡
            future_to_file = {}
            for file in files:
                try:
                    oss_path = self._get_oss_path(file, oss_root, batch_date)
                    future = executor.submit(self._upload_file, file, oss_path, retry_times)
                    future_to_file[future] = (file, oss_path)
                except Exception as e:
                    # ç”ŸæˆOSSè·¯å¾„å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡è¯¥æ–‡ä»¶
                    self.logger.error(f"âœ— è·³è¿‡æ–‡ä»¶(ç”ŸæˆOSSè·¯å¾„å¤±è´¥): {file.name} - {str(e)}")
                    failed_count += 1

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for i, future in enumerate(as_completed(future_to_file), 1):
                file, oss_path = future_to_file[future]
                try:
                    result = future.result()
                    if result['success']:
                        if result['skipped']:
                            skipped_count += 1
                        else:
                            success_count += 1
                    else:
                        failed_count += 1
                except Exception as e:
                    self.logger.error(f"âœ— ä¸Šä¼ å¼‚å¸¸: {file.name} - {str(e)}")
                    failed_count += 1

                # æ˜¾ç¤ºè¿›åº¦ - æ¯å¤„ç†ä¸€ä¸ªæ–‡ä»¶éƒ½æ˜¾ç¤ºè¿›åº¦
                progress_percent = (i / len(files)) * 100
                self.logger.info(f"è¿›åº¦: {i}/{len(files)} ({progress_percent:.1f}%) - æ–°ä¸Šä¼ : {success_count}, è·³è¿‡: {skipped_count}, å¤±è´¥: {failed_count}")

        # è¾“å‡ºç»Ÿè®¡
        self.logger.info("=" * 60)
        self.logger.info("ä¸Šä¼ å®Œæˆ")
        self.logger.info(f"æ€»æ–‡ä»¶æ•°: {len(files)}")
        self.logger.info(f"æ–°ä¸Šä¼ : {success_count}")
        self.logger.info(f"è·³è¿‡(å·²å­˜åœ¨): {skipped_count}")
        self.logger.info(f"å¤±è´¥: {failed_count}")
        self.logger.info("=" * 60)

    def run(self):
        """è¿è¡Œä¸Šä¼ ä»»åŠ¡"""
        try:
            # è·å–é…ç½®
            upload_settings = self.config['upload_settings']
            oss_root = Path(upload_settings['oss_root'])

            self.logger.info("=" * 60)
            self.logger.info("OSS ä¸Šä¼ ä»»åŠ¡é…ç½®")
            self.logger.info(f"ä¸Šä¼ æ ¹ç›®å½•: {oss_root}")
            self.logger.info(f"OSS Bucket: {self.config['aliyun_oss']['bucket_name']}")
            self.logger.info(f"OSS Endpoint: {self.config['aliyun_oss']['endpoint']}")
            self.logger.info("=" * 60)

            if not oss_root.exists():
                self.logger.error(f"âœ— OSS æ ¹ç›®å½•ä¸å­˜åœ¨: {oss_root}")
                return

            # ç»Ÿä¸€ç¡®å®šæ‰¹æ¬¡æ—¥æœŸç”¨äºå½’æ¡£å’Œä¸Šä¼ è·¯å¾„
            batch_date = self._get_batch_date(oss_root)
            if batch_date is None:
                self.logger.error("âœ— æ— æ³•ä»ç›®å½•ä¸­æå–æ‰¹æ¬¡æ—¥æœŸï¼Œä¸Šä¼ ç»ˆæ­¢")
                return

            # å…ˆå°†ç›®å½•æ‰“åŒ…æˆ 7z å‹ç¼©åŒ…ï¼šoss_root_yyyymmdd.7z
            archive_path = self._create_7z_archive(oss_root, batch_date)

            # ä¸Šä¼ å‹ç¼©åŒ…ï¼ˆæŒ‰ yyyy/yyyymmdd/oss_root_yyyymmdd.7z å­˜æ”¾ï¼‰
            self.upload_files([archive_path], oss_root, max_workers=1)

        except Exception as e:
            self.logger.error("=" * 60)
            self.logger.error(f"âœ— ä¸Šä¼ ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
            self.logger.error("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("é˜¿é‡Œäº‘ OSS æ–‡ä»¶ä¸Šä¼ å·¥å…·")
    print("=" * 60)
    print()

    try:
        # åˆ›å»ºä¸Šä¼ å™¨
        uploader = OSSUploader()

        # è¿è¡Œä¸Šä¼ 
        uploader.run()

        print()
        print("=" * 60)
        print("âœ“ ä¸Šä¼ ä»»åŠ¡å®Œæˆ")
        print("è¯¦ç»†æ—¥å¿—è¯·æŸ¥çœ‹: oss_upload.log")
        print("=" * 60)
    except Exception as e:
        print()
        print("=" * 60)
        print(f"âœ— ä¸Šä¼ ä»»åŠ¡å¤±è´¥: {str(e)}")
        print("è¯¦ç»†æ—¥å¿—è¯·æŸ¥çœ‹: oss_upload.log")
        print("=" * 60)
        sys.exit(1)


if __name__ == "__main__":
    main()

